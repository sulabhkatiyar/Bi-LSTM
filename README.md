# Bi-LSTM
## Partial re-implementation of Image Captioning with Deep Bidirectional LSTMs


### Introduction:
This model is similar to Bi-LSTM model proposed in _Image Captioning with Deep Bidirectional LSTMs_ published in _24th ACM international conference on Multimedia_ [[link]](https://dl.acm.org/doi/abs/10.1145/2964284.2964299) . An updated paper was published in journal ACM TOMM [[link]](https://dl.acm.org/doi/abs/10.1145/3115432).
There are following differences in our implementation:
1. I have not used Data Augmentation in this implementation. However, I have included options for horizontal and vertical data augmentation in the code which can be used by setting use_data_augmentation = True in train.py.
2. I have used batch size of 32 for all experiments and learning rate of 0.0001.
3. I have used VGG-16 CNN for image feature extraction whereas the authors used both AlexNet and VGG-16 for experiments.
4. Since both forward and backward LSTMs are trained for caption generation, I have experimented with both the inference strategy used in the paper (where the most likely sentence generated by forward or backward LSTMs is used as caption) and separate inference with backward and forward LSTMs.

